\documentclass[12pt, fleqn]{article}

\usepackage[utf8]{inputenc}
\usepackage[T2A]{fontenc}
\usepackage{amssymb, amsmath, mathrsfs, amsthm}
\usepackage[russian]{babel}
\usepackage{graphicx}
\usepackage[footnotesize]{caption2}
\usepackage{indentfirst}
\usepackage[hidelinks]{hyperref}
\usepackage{multirow}
\usepackage{pgfplotstable}
\usepackage{makecell}
%\usepackage[ruled,section]{algorithm}
%\usepackage[noend]{algorithmic}
%\usepackage[all]{xy}

% Параметры страницы
\textheight=24cm
\textwidth=16cm
\oddsidemargin=5mm
\evensidemargin=-5mm
\marginparwidth=36pt
\topmargin=-1cm
\footnotesep=3ex
%\flushbottom
\raggedbottom
\tolerance 3000
% подавить эффект "висячих стpок"
\clubpenalty=10000
\widowpenalty=10000
\renewcommand{\baselinestretch}{1.1}
\renewcommand{\baselinestretch}{1.5} %для печати с большим интервалом

% Дополнительные команды для личных обозначений
\newcommand{\expectation}{\mathop{\mathbb{E}}}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\loss}{\mathop{\mathcal{L}}}
\newcommand{\mse}{\mathop{MSE}}
\newcommand{\scalarproduct}[1]{\langle #1 \rangle}

\newcommand{\predictionfunction}{\hat{f}}
\newcommand{\ensemblefunction}{a}
\newcommand{\optimizationmethodfunction}{\tilde{f}}
\newcommand{\distinguishparameter}{\alpha}
\newcommand{\objects}{X}
\newcommand{\results}{Y}
\newcommand{\predictedobjects}{\widehat{\objects}}

\newcommand{\numberobjects}{N}
\newcommand{\numberpredictionfunctions}{M}

\newcommand{\for}[3]{\sum\limits_{#1 = #2}^{#3}}  % Usage: \for{index}{begin}{end}
\newcommand{\forn}[2]{\for{#1}{1}{#2}}  % Usage: \forn{index}{end}

\newcommand{\many}[3]{#1 1 #2, #1 2 #2, \dots, #1 #3 #2}  % Usage: \many{prefix}{suffix}{end}

\newcommand{\reference}[1]{(\hyperref[#1]{\ref{#1}})}

\newcommand{\ensemblefunctionfull}{\ensemblefunction(\many{\predictionfunction_}{(x)}{\numberpredictionfunctions})}

\begin{document}

\begin{titlepage}
\begin{center}
    Московский государственный университет имени М. В. Ломоносова

    \bigskip
    \includegraphics[width=50mm]{msu.eps}

    \bigskip
    Факультет Вычислительной Математики и Кибернетики\\
    Кафедра Математических Методов Прогнозирования\\[10mm]

    \textsf{\large\bfseries
        <<Сравнение скорости вычисления собственных значений\\ положительно определённых матриц\\ при помощи QR алгоритма>>
    }\\[20mm]

    \begin{flushright}
        \parbox{0.5\textwidth}{
            Выполнил:\\
            студент 1 курса магистратуры 517 группы\\
            \emph{Королев Николай Сергеевич}\\[5mm]
            Преподаватель:\\
            канд. техн. наук, доцент\\
            \emph{Русол Андрей Владимирович}
        }
    \end{flushright}

    \vspace{\fill}
    Москва, 2021
\end{center}
\end{titlepage}

\newpage
\section{Постановка задачи}

Исследовать способы ускорения вычисления собственных значений положительно определённых матриц.

\subsection{Постановка задачи о вычислении собственных значений положительно определённой матрицы}
Дана положительно определённая матрица $A$ размера $n \times n$. Необходимо вычислить все $n$ её собственных значений $\lambda_1, \lambda_2, \dots, \lambda_n$.

\section{QR алгоритм}

Для нахождения всех собственных значений положительно определённой матрицы $A$ можно воспользоваться QR алгоритмом, который выглядит следующим образом:
\begin{enumerate}
	\item Обозначим $A_0 := A, \; k := 0$.
	\item Представить матрицу $A_k$ в виде произведения унитарной матрицы $Q_k$ и верхнетреугольной матрицы $R_k$. (Произвести QR разложение матрицы $A$) $Q_k R_k = A_k$
	\item Вычислить $A_{k+1} := R_k Q_k$
	\item Увеличить $k$ на единицу. $k := k + 1$
	\item Повторить шаги 2-4 до тех пор пока внедиагональные элементы матрицы $A_k$ не станут близкими к нулю.
	\item Значения на диагонали матрицы $A_k$ будут являться приближением собственными значениями матрицы $A$.
\end{enumerate}

\subsection{Доказательство корректности алгоритма}
Заметим, что все матрицы $A_k$ для $k = 0, 1, \dots$ являются подобными, т.к. \\
$A_{k + 1} = R_k Q_k = Q_k^{-1} Q_k R_k Q_k = Q_k^{-1} A_k Q_k = Q_k^T A_k Q_k$, а значит их собственные значения совпадают.

Также для положительно определённой матрицы $A$ известно \cite{QR}, что внедиагольные элементы матрицы $A_k$ будут стремиться к нулю при $k \to \infty$.

\section{Вычислительные эксперименты}

QR алгоритм был реализован тремя различными способами на языке Python 3 при помощи библиотеки Numpy для использования векторизации вычислений, после чего лучшая из имплементаций была ускорена при помощи JIT-компилятора Numba. Результаты измерений приведены в таблице \ref{tabular:results}.

\begin{center}
	\pgfplotstabletypeset[
	after row=\hline,
	col sep=semicolon,
	string type,
	columns/index/.style={column name={Имплементация
		}, column type={|c}},
	columns/10/.style={column name={$N = 10$}, column type={|c}},
	columns/20/.style={column name={$N = 20$}, column type={|c}},
	columns/30/.style={column name={$N = 30$}, column type={|c}},
	columns/40/.style={column name={$N = 40$}, column type={|c}},
	columns/50/.style={column name={$N = 50$}, column type={|c|}},
	every head row/.style={before row=\hline,after row=\hline},
	every last row/.style={after row=\hline},
	]{tables/results.csv}
	
	\begin{table}[h!]
		\caption{Время выполнения QR алгоритма различных имплементаций при различных размерах исходной матрицы в миллисекундах. В таблице приведено среднее время выполнения ± средне квадратичное отклонение времени по 7 запускам алгоритма.}
		\label{tabular:results}
	\end{table}
\end{center}

Все имплементации можно найти по адресу \footnote{\href{https://github.com/CrafterKolyan/eigenvalues-speed-comparison/blob/main/experiments/python/EigenValues.ipynb}{https://github.com/CrafterKolyan/eigenvalues-speed-comparison/blob/main/experiments/python/EigenValues.ipynb}}

\subsection{Анализ полученных результатов}

Полученные результаты показывают, что ансамбль промежуточных решений, построенных с использованием функции потерь \reference{equation:lossfunction}, способен достаточно серьёзно увеличивать обобщающую способность предсказания в сравнении с отдельными нейронными сетями.
Также замечен эффект улучшения качества отдельных нейронных сетей, построенных данным методом. Попробуем объяснить данный эффект. Предполагается, что он вызван тем, что в случае нахождения недостаточно минимизирующего параметра $\theta_1$ для нейронной сети $\predictionfunction_1(x, \theta_1)$, последующие нейронные сети находят параметры $\theta_i$ на отдалении от $\theta_1$. Следовательно, $\theta_i$ не находится в области $\theta_1$, в которой функция потерь не достигает своего минимального значения.

\section{Заключение}

В процессе выполнения работы были получены следующие результаты:
\begin{itemize}
\item Был разработан метод повышения эффективности обучения, основанный на ансамбле промежуточных решений.
\item Были проведены вычислительные эксперименты, которые показали возможную применимость данного метода для улучшения качества нейронных сетей, решающих задачу классификации на реальных данных.
\item В ходе выполнения эксперимента было замечено улучшения качества работы отдельных нейронных сетей.
\end{itemize}

\def\BibUrl#1.{}\def\BibAnnote#1.{}
%\def\BibUrl#1{\\{\footnotesize\tt\def~{\char126} http://#1}}
\bibliographystyle{gost71s}
\bibliography{bibliography}

\end{document}
